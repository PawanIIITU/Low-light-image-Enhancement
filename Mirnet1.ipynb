{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.6.0.66-cp36-abi3-win_amd64.whl (35.6 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from opencv-python) (1.21.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.6.0.66\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl (266.3 MB)\n",
      "     -------------------------------------- 266.3/266.3 MB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.28.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 18.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.3.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.1.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-14.0.6-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.11.23-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 15.9 MB/s eta 0:00:00\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 15.4 MB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.50.0-cp39-cp39-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 16.6 MB/s eta 0:00:00\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-win_amd64.whl (895 kB)\n",
      "     ------------------------------------- 895.9/895.9 kB 14.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "     ------------------------------------- 439.2/439.2 kB 28.6 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.14.1-py2.py3-none-any.whl (175 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\ravindra\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.3.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-22.11.23 gast-0.4.0 google-auth-2.14.1 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.50.0 keras-2.11.0 libclang-14.0.6 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.28.0 termcolor-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "H1-k5mIo0nFC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qxdL_dRGCJlw",
    "outputId": "e024d147-067c-482a-ebde-47679fb97cb0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tjLTHTS04Mj",
    "outputId": "77e657b7-7614-4936-9e97-62420a9ea783"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: <BatchDataset element_spec=(TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None))>\n",
      "Val Dataset: <BatchDataset element_spec=(TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "random.seed(10)\n",
    "\n",
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 4\n",
    "MAX_TRAIN_IMAGES = 300\n",
    "\n",
    "\n",
    "def read_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image.set_shape([None, None, 3])\n",
    "    image = tf.cast(image, dtype=tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "\n",
    "def random_crop(low_image, enhanced_image):\n",
    "    low_image_shape = tf.shape(low_image)[:2]\n",
    "    low_w = tf.random.uniform(\n",
    "        shape=(), maxval=low_image_shape[1] - IMAGE_SIZE + 1, dtype=tf.int32\n",
    "    )\n",
    "    low_h = tf.random.uniform(\n",
    "        shape=(), maxval=low_image_shape[0] - IMAGE_SIZE + 1, dtype=tf.int32\n",
    "    )\n",
    "    enhanced_w = low_w\n",
    "    enhanced_h = low_h\n",
    "    low_image_cropped = low_image[\n",
    "        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n",
    "    ]\n",
    "    enhanced_image_cropped = enhanced_image[\n",
    "        enhanced_h : enhanced_h + IMAGE_SIZE, enhanced_w : enhanced_w + IMAGE_SIZE\n",
    "    ]\n",
    "    return low_image_cropped, enhanced_image_cropped\n",
    "\n",
    "\n",
    "def load_data(low_light_image_path, enhanced_image_path):\n",
    "    low_light_image = read_image(low_light_image_path)\n",
    "    enhanced_image = read_image(enhanced_image_path)\n",
    "    low_light_image, enhanced_image = random_crop(low_light_image, enhanced_image)\n",
    "    return low_light_image, enhanced_image\n",
    "\n",
    "\n",
    "def get_dataset(low_light_images, enhanced_images):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((low_light_images, enhanced_images))\n",
    "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_low_light_images = sorted(glob(r\"C:\\Users\\Ravindra\\lol_dataset\\lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n",
    "train_enhanced_images = sorted(glob(r\"C:\\Users\\Ravindra\\lol_dataset\\lol_dataset/our485/high/*\"))[:MAX_TRAIN_IMAGES]\n",
    "\n",
    "val_low_light_images = sorted(glob(r\"C:\\Users\\Ravindra\\lol_dataset\\lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n",
    "val_enhanced_images = sorted(glob(r\"C:\\Users\\Ravindra\\lol_dataset\\lol_dataset/our485/high/*\"))[MAX_TRAIN_IMAGES:]\n",
    "\n",
    "test_low_light_images = sorted(glob(r\"C:\\Users\\Ravindra\\lol_dataset\\lol_dataset/eval15/low/*\"))\n",
    "test_enhanced_images = sorted(glob(r\"C:\\Users\\Ravindra\\lol_dataset\\lol_dataset/eval15/high/*\"))\n",
    "\n",
    "\n",
    "train_dataset = get_dataset(train_low_light_images, train_enhanced_images)\n",
    "val_dataset = get_dataset(val_low_light_images, val_enhanced_images)\n",
    "\n",
    "\n",
    "print(\"Train Dataset:\", train_dataset)\n",
    "print(\"Val Dataset:\", val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j51puB0R1BDC"
   },
   "outputs": [],
   "source": [
    "\n",
    "def selective_kernel_feature_fusion(\n",
    "    multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3\n",
    "):\n",
    "    channels = list(multi_scale_feature_1.shape)[-1]\n",
    "    combined_feature = layers.Add()(\n",
    "        [multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3]\n",
    "    )\n",
    "    gap = layers.GlobalAveragePooling2D()(combined_feature)\n",
    "    channel_wise_statistics = tf.reshape(gap, shape=(-1, 1, 1, channels))\n",
    "    compact_feature_representation = layers.Conv2D(\n",
    "        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n",
    "    )(channel_wise_statistics)\n",
    "    feature_descriptor_1 = layers.Conv2D(\n",
    "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
    "    )(compact_feature_representation)\n",
    "    feature_descriptor_2 = layers.Conv2D(\n",
    "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
    "    )(compact_feature_representation)\n",
    "    feature_descriptor_3 = layers.Conv2D(\n",
    "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
    "    )(compact_feature_representation)\n",
    "    feature_1 = multi_scale_feature_1 * feature_descriptor_1\n",
    "    feature_2 = multi_scale_feature_2 * feature_descriptor_2\n",
    "    feature_3 = multi_scale_feature_3 * feature_descriptor_3\n",
    "    aggregated_feature = layers.Add()([feature_1, feature_2, feature_3])\n",
    "    return aggregated_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XoLrKz2T1Ha7"
   },
   "outputs": [],
   "source": [
    "\n",
    "def spatial_attention_block(input_tensor):\n",
    "    average_pooling = tf.reduce_max(input_tensor, axis=-1)\n",
    "    average_pooling = tf.expand_dims(average_pooling, axis=-1)\n",
    "    max_pooling = tf.reduce_mean(input_tensor, axis=-1)\n",
    "    max_pooling = tf.expand_dims(max_pooling, axis=-1)\n",
    "    concatenated = layers.Concatenate(axis=-1)([average_pooling, max_pooling])\n",
    "    feature_map = layers.Conv2D(1, kernel_size=(1, 1))(concatenated)\n",
    "    feature_map = tf.nn.sigmoid(feature_map)\n",
    "    return input_tensor * feature_map\n",
    "\n",
    "\n",
    "def channel_attention_block(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    average_pooling = layers.GlobalAveragePooling2D()(input_tensor)\n",
    "    feature_descriptor = tf.reshape(average_pooling, shape=(-1, 1, 1, channels))\n",
    "    feature_activations = layers.Conv2D(\n",
    "        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n",
    "    )(feature_descriptor)\n",
    "    feature_activations = layers.Conv2D(\n",
    "        filters=channels, kernel_size=(1, 1), activation=\"sigmoid\"\n",
    "    )(feature_activations)\n",
    "    return input_tensor * feature_activations\n",
    "\n",
    "\n",
    "def dual_attention_unit_block(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    feature_map = layers.Conv2D(\n",
    "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "    )(input_tensor)\n",
    "    feature_map = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n",
    "        feature_map\n",
    "    )\n",
    "    channel_attention = channel_attention_block(feature_map)\n",
    "    spatial_attention = spatial_attention_block(feature_map)\n",
    "    concatenation = layers.Concatenate(axis=-1)([channel_attention, spatial_attention])\n",
    "    concatenation = layers.Conv2D(channels, kernel_size=(1, 1))(concatenation)\n",
    "    return layers.Add()([input_tensor, concatenation])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8YFiX_SA1NeR"
   },
   "outputs": [],
   "source": [
    "# Recursive Residual Modules\n",
    "\n",
    "\n",
    "def down_sampling_module(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n",
    "        input_tensor\n",
    "    )\n",
    "    main_branch = layers.Conv2D(\n",
    "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "    )(main_branch)\n",
    "    main_branch = layers.MaxPooling2D()(main_branch)\n",
    "    main_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(main_branch)\n",
    "    skip_branch = layers.MaxPooling2D()(input_tensor)\n",
    "    skip_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(skip_branch)\n",
    "    return layers.Add()([skip_branch, main_branch])\n",
    "\n",
    "\n",
    "def up_sampling_module(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n",
    "        input_tensor\n",
    "    )\n",
    "    main_branch = layers.Conv2D(\n",
    "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "    )(main_branch)\n",
    "    main_branch = layers.UpSampling2D()(main_branch)\n",
    "    main_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(main_branch)\n",
    "    skip_branch = layers.UpSampling2D()(input_tensor)\n",
    "    skip_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(skip_branch)\n",
    "    return layers.Add()([skip_branch, main_branch])\n",
    "\n",
    "\n",
    "# MRB Block\n",
    "def multi_scale_residual_block(input_tensor, channels):\n",
    "    # features\n",
    "    level1 = input_tensor\n",
    "    level2 = down_sampling_module(input_tensor)\n",
    "    level3 = down_sampling_module(level2)\n",
    "    # DAU\n",
    "    level1_dau = dual_attention_unit_block(level1)\n",
    "    level2_dau = dual_attention_unit_block(level2)\n",
    "    level3_dau = dual_attention_unit_block(level3)\n",
    "    # SKFF\n",
    "    level1_skff = selective_kernel_feature_fusion(\n",
    "        level1_dau,\n",
    "        up_sampling_module(level2_dau),\n",
    "        up_sampling_module(up_sampling_module(level3_dau)),\n",
    "    )\n",
    "    level2_skff = selective_kernel_feature_fusion(\n",
    "        down_sampling_module(level1_dau), level2_dau, up_sampling_module(level3_dau)\n",
    "    )\n",
    "    level3_skff = selective_kernel_feature_fusion(\n",
    "        down_sampling_module(down_sampling_module(level1_dau)),\n",
    "        down_sampling_module(level2_dau),\n",
    "        level3_dau,\n",
    "    )\n",
    "    # DAU 2\n",
    "    level1_dau_2 = dual_attention_unit_block(level1_skff)\n",
    "    level2_dau_2 = up_sampling_module((dual_attention_unit_block(level2_skff)))\n",
    "    level3_dau_2 = up_sampling_module(\n",
    "        up_sampling_module(dual_attention_unit_block(level3_skff))\n",
    "    )\n",
    "    # SKFF 2\n",
    "    skff_ = selective_kernel_feature_fusion(level1_dau_2, level3_dau_2, level3_dau_2)\n",
    "    conv = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(skff_)\n",
    "    return layers.Add()([input_tensor, conv])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mX9vv4FU1Ye2"
   },
   "source": [
    "Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lzyaCDTk1TR4"
   },
   "outputs": [],
   "source": [
    "\n",
    "def recursive_residual_group(input_tensor, num_mrb, channels):\n",
    "    conv1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n",
    "    for _ in range(num_mrb):\n",
    "        conv1 = multi_scale_residual_block(conv1, channels)\n",
    "    conv2 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(conv1)\n",
    "    return layers.Add()([conv2, input_tensor])\n",
    "\n",
    "\n",
    "def mirnet_model(num_rrg, num_mrb, channels):\n",
    "    input_tensor = keras.Input(shape=[None, None, 3])\n",
    "    x1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n",
    "    for _ in range(num_rrg):\n",
    "        x1 = recursive_residual_group(x1, num_mrb, channels)\n",
    "    conv = layers.Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x1)\n",
    "    output_tensor = layers.Add()([input_tensor, conv])\n",
    "    return keras.Model(input_tensor, output_tensor)\n",
    "\n",
    "\n",
    "model = mirnet_model(num_rrg=3, num_mrb=2, channels=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7VjbBqrvauX"
   },
   "outputs": [],
   "source": [
    "model.save(\"./model\"+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cM5PftP91bZP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def charbonnier_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.sqrt(tf.square(y_true - y_pred) + tf.square(1e-3)))\n",
    "\n",
    "\n",
    "def peak_signal_noise_ratio(y_true, y_pred):\n",
    "    return tf.image.psnr(y_pred, y_true, max_val=255.0)\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=charbonnier_loss, metrics=[peak_signal_noise_ratio]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10,\n",
    "    callbacks=[\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_peak_signal_noise_ratio\",\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            min_delta=1e-7,\n",
    "            mode=\"max\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(history.history[\"peak_signal_noise_ratio\"], label=\"train_psnr\")\n",
    "plt.plot(history.history[\"val_peak_signal_noise_ratio\"], label=\"val_psnr\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"PSNR\")\n",
    "plt.title(\"Train and Validation PSNR Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_nM2rSJG1i48"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(images, titles, figure_size=(12, 12)):\n",
    "    fig = plt.figure(figsize=figure_size)\n",
    "    for i in range(len(images)):\n",
    "        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n",
    "        _ = plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def infer(original_image):\n",
    "    image = keras.preprocessing.image.img_to_array(original_image)\n",
    "    image = image.astype(\"float32\") / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    output = loaded_model.predict(image)\n",
    "    output_image = output[0] * 255.0\n",
    "    output_image = output_image.clip(0, 255)\n",
    "    output_image = output_image.reshape(\n",
    "        (np.shape(output_image)[0], np.shape(output_image)[1], 3)\n",
    "    )\n",
    "    output_image = Image.fromarray(np.uint8(output_image))\n",
    "    original_image = Image.fromarray(np.uint8(original_image))\n",
    "    return output_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUbdyf8s1oOf"
   },
   "outputs": [],
   "source": [
    "for low_light_image in random.sample(test_low_light_images, 6):\n",
    "    original_image = Image.open(low_light_image)\n",
    "    enhanced_image = infer(original_image)\n",
    "    plot_results(\n",
    "        [original_image, ImageOps.autocontrast(original_image), enhanced_image],\n",
    "        [\"Original\", \"PIL Autocontrast\", \"MIRNet Enhanced\"],\n",
    "        (20, 12),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
